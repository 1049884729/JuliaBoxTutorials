{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC in Julia\n",
    "## Lecture I - π in many ways\n",
    "### Recap of CPU solution\n",
    "\n",
    "Yesterday, you computed approximations to π by Monte Carlo simulations by utilizing that if $(X_i)$ and $(Y_i)$ are independent uniformly distributed random variables on the unit interval then the Law of Large Numbers gives\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}\\{X_i^2 + Y_i^2 < 1\\} \\overset{P}{\\longrightarrow} \n",
    "E \\mathbb{1}\\{X_i^2 + Y_i^2 < 1\\} = \\frac{\\pi}{4}.\n",
    "$$\n",
    "\n",
    "In this notebook, we will use this result to introduce different modes of parallelism in Julia. First, we will redo the calculation on CPU and use that as a reference for comparison when converting the implementation to GPU, distributed and threaded versions.\n",
    "\n",
    "The version we will use here is based on arrays and higher order functions such as map and reduce (here as a sum). As we will see by the end, this version might not always be the fastest possible but it will be competitive and it also has the advantage that it looks very similar to the math behind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simulate_π_cpu (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate_π_cpu(n) = 4*sum(map((x, y) -> x^2 + y^2 < 1, rand(Float32, n), rand(Float32, n)))/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll load `BenchmarkTools` for the timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  544.984 ns (9 allocations: 720 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "@btime simulate_π_cpu(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For for `n = 10`, the CPU version is 300-400 times faster than the GPU version. Let's see the effecting using a ten times larger `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.240 μs (9 allocations: 1.61 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.36"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime simulate_π_cpu(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CPU version scales roughly linearly whereas the GPU version is roughly unaffected. This is explained by the large constant costs of memory operations on the GPU. Now lets try `n = 10^6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7.409 ms (12 allocations: 8.58 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.141324"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime simulate_π_cpu(10^6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a better idea about how this scales, we'll collect some more timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = 3:23\n",
    "cpu_timings = [minimum(Base.@elapsed simulate_π_cpu(2^n) for i in 1:5) for n in ns];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and define a small convenience function for ploting the timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <script class='js-collapse-script'>\n",
       "        var curMatch =\n",
       "            window.location.href\n",
       "            .match(/(.*?)\\/notebooks\\/.*\\.ipynb/);\n",
       "\n",
       "        curMatch = curMatch ||\n",
       "            window.location.href\n",
       "            .match(/(.*?)\\/apps\\/.*\\.ipynb/);\n",
       "\n",
       "        if ( curMatch ) {\n",
       "            $('head').append('<base href=\"' + curMatch[1] + '/\">');\n",
       "        }\n",
       "    </script>\n"
      ],
      "text/plain": [
       "HTML{String}(\"    <script class='js-collapse-script'>\\n        var curMatch =\\n            window.location.href\\n            .match(/(.*?)\\\\/notebooks\\\\/.*\\\\.ipynb/);\\n\\n        curMatch = curMatch ||\\n            window.location.href\\n            .match(/(.*?)\\\\/apps\\\\/.*\\\\.ipynb/);\\n\\n        if ( curMatch ) {\\n            \\$('head').append('<base href=\\\"' + curMatch[1] + '/\\\">');\\n        }\\n    </script>\\n\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script class='js-collapse-script' src='/assetserver/528251b1ff164bb91886525e074d4435e4f7eab2-assets/webio/dist/bundle.js'></script>"
      ],
      "text/plain": [
       "HTML{String}(\"<script class='js-collapse-script' src='/assetserver/528251b1ff164bb91886525e074d4435e4f7eab2-assets/webio/dist/bundle.js'></script>\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script class='js-collapse-script' src='/assetserver/528251b1ff164bb91886525e074d4435e4f7eab2-assets/providers/ijulia_setup.js'></script>"
      ],
      "text/plain": [
       "HTML{String}(\"<script class='js-collapse-script' src='/assetserver/528251b1ff164bb91886525e074d4435e4f7eab2-assets/providers/ijulia_setup.js'></script>\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "  <script class='js-collapse-script'>\n",
       "    $('.js-collapse-script').parent('.output_subarea').css('padding', '0');\n",
       "  </script>\n"
      ],
      "text/plain": [
       "HTML{String}(\"  <script class='js-collapse-script'>\\n    \\$('.js-collapse-script').parent('.output_subarea').css('padding', '0');\\n  </script>\\n\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "plot_timings (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PlotlyWebIO\n",
    "function plot_timings(sz::AbstractVector, times::Vector...; names = Vector{String} = [\"t$i\" for i in 1:length(times)])\n",
    "    n = length(times)\n",
    "    \n",
    "    if length(times) != n\n",
    "        throw(DimensionMismatch(\"\"))\n",
    "    end\n",
    "\n",
    "    ps = vcat(map(zip(times, names)) do tn\n",
    "        scatter(\n",
    "            x = 2.^sz,\n",
    "            y = tn[1],\n",
    "            name = tn[2]\n",
    "        )\n",
    "    end...)\n",
    "\n",
    "    ps12 = hcat(\n",
    "        Plot(ps,\n",
    "            Layout(\n",
    "                title = \"Timings on linear scales\"\n",
    "            )\n",
    "        ),\n",
    "        Plot(ps,\n",
    "            Layout(\n",
    "                title = \"Timings on logarithmic scales\",\n",
    "                xaxis_type = \"log\",\n",
    "                yaxis_type = \"log\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return PlotlyWebIO.WebIOPlot(ps12)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='display:none'></div><unsafe-script style='display:none'>\n",
       "WebIO.mount(this.previousSibling,{&quot;props&quot;:{},&quot;nodeType&quot;:&quot;Scope&quot;,&quot;type&quot;:&quot;node&quot;,&quot;instanceArgs&quot;:{&quot;imports&quot;:{&quot;data&quot;:[{&quot;name&quot;:&quot;Plotly&quot;,&quot;type&quot;:&quot;js&quot;,&quot;url&quot;:&quot;/assetserver/df345f3ad972d7ff84a1db2dc9a91e92993f10ca-plotly-latest.min.js&quot;},{&quot;name&quot;:null,&quot;type&quot;:&quot;js&quot;,&quot;url&quot;:&quot;/assetserver/96b46ae3f393c44534f2e95b270ff2970bf2730d-plotly_webio_bundle.js&quot;}],&quot;type&quot;:&quot;async_block&quot;},&quot;id&quot;:&quot;scope-d75517cd-7907-4818-aae0-86eaf618e25d&quot;,&quot;handlers&quot;:{&quot;_promises&quot;:{&quot;importsLoaded&quot;:[(function (Plotly){var gd=this.dom.querySelector(&quot;#plot-049e068a-b6e3-4749-bc8a-d748e0afc9a0&quot;); this.plotElem=gd; this.Plotly=Plotly; (window.Blink!==undefined) ? (gd.style.width=&quot;100%&quot;, gd.style.height=&quot;100vh&quot;, gd.style.marginLeft=&quot;0%&quot;, gd.style.marginTop=&quot;0vh&quot;) : undefined; window.onresize=(function (){return Plotly.Plots.resize(gd)}); Plotly.newPlot(gd,[{&quot;xaxis&quot;:&quot;x1&quot;,&quot;y&quot;:[4.94e-7,6.26e-7,8.33e-7,1.059e-6,1.551e-6,3.442e-6,6.388e-6,9.24e-6,1.7362e-5,3.9534e-5,7.8854e-5,0.000168271,0.000255836,0.000487113,0.000980942,0.001954024,0.003919791,0.007842749,0.015871381,0.034802117,0.093679037],&quot;type&quot;:&quot;scatter&quot;,&quot;name&quot;:&quot;CPU&quot;,&quot;yaxis&quot;:&quot;y1&quot;,&quot;x&quot;:[8,16,32,64,128,256,512,1024,2048,4096,8192,16384,32768,65536,131072,262144,524288,1048576,2097152,4194304,8388608]},{&quot;xaxis&quot;:&quot;x2&quot;,&quot;y&quot;:[4.94e-7,6.26e-7,8.33e-7,1.059e-6,1.551e-6,3.442e-6,6.388e-6,9.24e-6,1.7362e-5,3.9534e-5,7.8854e-5,0.000168271,0.000255836,0.000487113,0.000980942,0.001954024,0.003919791,0.007842749,0.015871381,0.034802117,0.093679037],&quot;type&quot;:&quot;scatter&quot;,&quot;name&quot;:&quot;CPU&quot;,&quot;yaxis&quot;:&quot;y2&quot;,&quot;x&quot;:[8,16,32,64,128,256,512,1024,2048,4096,8192,16384,32768,65536,131072,262144,524288,1048576,2097152,4194304,8388608]}],{&quot;xaxis1&quot;:{&quot;domain&quot;:[0.0,0.45],&quot;anchor&quot;:&quot;y1&quot;},&quot;yaxis1&quot;:{&quot;domain&quot;:[0.0,1.0],&quot;anchor&quot;:&quot;x1&quot;},&quot;xaxis2&quot;:{&quot;type&quot;:&quot;log&quot;,&quot;domain&quot;:[0.55,1.0],&quot;anchor&quot;:&quot;y2&quot;},&quot;annotations&quot;:[{&quot;yanchor&quot;:&quot;bottom&quot;,&quot;xanchor&quot;:&quot;center&quot;,&quot;y&quot;:1.0,&quot;font&quot;:{&quot;size&quot;:16},&quot;showarrow&quot;:false,&quot;yref&quot;:&quot;paper&quot;,&quot;text&quot;:&quot;Timings on linear scales&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x&quot;:0.225},{&quot;yanchor&quot;:&quot;bottom&quot;,&quot;xanchor&quot;:&quot;center&quot;,&quot;y&quot;:1.0,&quot;font&quot;:{&quot;size&quot;:16},&quot;showarrow&quot;:false,&quot;yref&quot;:&quot;paper&quot;,&quot;text&quot;:&quot;Timings on logarithmic scales&quot;,&quot;xref&quot;:&quot;paper&quot;,&quot;x&quot;:0.775}],&quot;margin&quot;:{&quot;l&quot;:50,&quot;b&quot;:50,&quot;r&quot;:50,&quot;t&quot;:60},&quot;yaxis2&quot;:{&quot;type&quot;:&quot;log&quot;,&quot;domain&quot;:[0.0,1.0],&quot;anchor&quot;:&quot;x2&quot;}},{&quot;showLink&quot;:false}).then((function (gd){return Plotly.toImage(gd,{&quot;format&quot;:&quot;svg&quot;})})).then((function (data){var svg_data=data.replace(&quot;data:image/svg+xml,&quot;,&quot;&quot;); return WebIO.setval({&quot;name&quot;:&quot;svg&quot;,&quot;scope&quot;:&quot;scope-d75517cd-7907-4818-aae0-86eaf618e25d&quot;,&quot;id&quot;:&quot;ob_01&quot;,&quot;type&quot;:&quot;observable&quot;},decodeURIComponent(svg_data))})); gd.on(&quot;plotly_hover&quot;,(function (data){var filtered_data=WebIO.CommandSets.Plotly.filterEventData(gd,data,&quot;hover&quot;); return !(filtered_data.isnil) ? (WebIO.setval({&quot;name&quot;:&quot;hover&quot;,&quot;scope&quot;:&quot;scope-d75517cd-7907-4818-aae0-86eaf618e25d&quot;,&quot;id&quot;:&quot;ob_02&quot;,&quot;type&quot;:&quot;observable&quot;},filtered_data.out)) : undefined})); gd.on(&quot;plotly_unhover&quot;,(function (){return WebIO.setval({&quot;name&quot;:&quot;hover&quot;,&quot;scope&quot;:&quot;scope-d75517cd-7907-4818-aae0-86eaf618e25d&quot;,&quot;id&quot;:&quot;ob_02&quot;,&quot;type&quot;:&quot;observable&quot;},{})})); gd.on(&quot;plotly_selected&quot;,(function (data){var filtered_data=WebIO.CommandSets.Plotly.filterEventData(gd,data,&quot;selected&quot;); return !(filtered_data.isnil) ? (WebIO.setval({&quot;name&quot;:&quot;selected&quot;,&quot;scope&quot;:&quot;scope-d75517cd-7907-4818-aae0-86eaf618e25d&quot;,&quot;id&quot;:&quot;ob_03&quot;,&quot;type&quot;:&quot;observable&quot;},filtered_data.out)) : undefined})); gd.on(&quot;plotly_deselect&quot;,(function (){return WebIO.setval({&quot;name&quot;:&quot;selected&quot;,&quot;scope&quot;:&quot;scope-d75517cd-7907-4818-aae0-86eaf618e25d&quot;,&quot;id&quot;:&quot;ob_03&quot;,&quot;type&quot;:&quot;observable&quot;},{})})); gd.on(&quot;plotly_relayout&quot;,(function (data){var filtered_data=WebIO.CommandSets.Plotly.filterEventData(gd,data,&quot;relayout&quot;); return !(filtered_data.isnil) ? (WebIO.setval({&quot;name&quot;:&quot;relayout&quot;,&quot;scope&quot;:&quot;scope-d75517cd-7907-4818-aae0-86eaf618e25d&quot;,&quot;id&quot;:&quot;ob_05&quot;,&quot;type&quot;:&quot;observable&quot;},filtered_data.out)) : undefined})); return gd.on(&quot;plotly_click&quot;,(function (data){var filtered_data=WebIO.CommandSets.Plotly.filterEventData(gd,data,&quot;click&quot;); return !(filtered_data.isnil) ? (WebIO.setval({&quot;name&quot;:&quot;click&quot;,&quot;scope&quot;:&quot;scope-d75517cd-7907-4818-aae0-86eaf618e25d&quot;,&quot;id&quot;:&quot;ob_04&quot;,&quot;type&quot;:&quot;observable&quot;},filtered_data.out)) : undefined}))})]},&quot;_commands&quot;:[(function (args){var fn=args.shift(); var elem=this.plotElem; var Plotly=this.Plotly; args.unshift(elem); return Plotly[fn].apply(this,args).then((function (gd){return Plotly.toImage(elem,{&quot;format&quot;:&quot;svg&quot;})})).then((function (data){var svg_data=data.replace(&quot;data:image/svg+xml,&quot;,&quot;&quot;); return WebIO.setval({&quot;name&quot;:&quot;svg&quot;,&quot;scope&quot;:&quot;scope-d75517cd-7907-4818-aae0-86eaf618e25d&quot;,&quot;id&quot;:&quot;ob_01&quot;,&quot;type&quot;:&quot;observable&quot;},decodeURIComponent(svg_data))}))})]},&quot;systemjs_options&quot;:null,&quot;observables&quot;:{&quot;hover&quot;:{&quot;sync&quot;:false,&quot;id&quot;:&quot;ob_02&quot;,&quot;value&quot;:{}},&quot;selected&quot;:{&quot;sync&quot;:false,&quot;id&quot;:&quot;ob_03&quot;,&quot;value&quot;:{}},&quot;click&quot;:{&quot;sync&quot;:false,&quot;id&quot;:&quot;ob_04&quot;,&quot;value&quot;:{}},&quot;relayout&quot;:{&quot;sync&quot;:false,&quot;id&quot;:&quot;ob_05&quot;,&quot;value&quot;:{}},&quot;svg&quot;:{&quot;sync&quot;:false,&quot;id&quot;:&quot;ob_01&quot;,&quot;value&quot;:&quot;&quot;},&quot;_commands&quot;:{&quot;sync&quot;:false,&quot;id&quot;:&quot;ob_06&quot;,&quot;value&quot;:[]}}},&quot;children&quot;:[{&quot;props&quot;:{&quot;id&quot;:&quot;plot-049e068a-b6e3-4749-bc8a-d748e0afc9a0&quot;,&quot;events&quot;:{}},&quot;nodeType&quot;:&quot;DOM&quot;,&quot;type&quot;:&quot;node&quot;,&quot;instanceArgs&quot;:{&quot;namespace&quot;:&quot;html&quot;,&quot;tag&quot;:&quot;div&quot;},&quot;children&quot;:[]}]})</unsafe-script>"
      ],
      "text/plain": [
       "data: [\n",
       "  \"scatter with fields name, type, x, xaxis, y, and yaxis\",\n",
       "  \"scatter with fields name, type, x, xaxis, y, and yaxis\"\n",
       "]\n",
       "\n",
       "layout: \"layout with fields annotations, margin, xaxis1, xaxis2, yaxis1, and yaxis2\"\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_timings(ns, cpu_timings, names = [\"CPU\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUs in Julia\n",
    "\n",
    "For this section to work, the notebook should be run on where a NVidia GPU is available. Typically, an easy way of querying for available GPUs (devices) is to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  3 02:46:41 2018       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 390.59                 Driver Version: 390.59                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 23%   39C    P2    57W / 250W |   3744MiB / 11178MiB |      2%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 30%   52C    P2    71W / 250W |  10664MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE...  Off  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    53W / 250W |  16278MiB / 16280MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE...  Off  | 00000000:10:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    31W / 250W |    951MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-PCIE...  Off  | 00000000:11:00.0 Off |                    0 |\n",
      "| N/A   39C    P0   ERR! / 250W |     11MiB / 16160MiB |     11%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     28698      C   /home/avik-pal/julia/usr/bin/julia          3063MiB |\n",
      "|    0     29086      C   ../../julia/julia                            671MiB |\n",
      "|    1      6435      C   ../../julia/julia                          10487MiB |\n",
      "|    1     11725      C   julia07                                      151MiB |\n",
      "|    2     14931      C   /home/avik-pal/julia/julia                 15725MiB |\n",
      "|    2     22269      C   ../../julia/julia                            543MiB |\n",
      "|    3     11725      C   julia07                                      939MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    ";nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should show a table listing the available GPUs as well as a list of processes running on the GPU.\n",
    "\n",
    "Julia is able to utilize NVidia GPUs through a set of packages of varying level of abstraction. For many usecases, the low level packages can be ignored but it is important to emphasize that the high level package are mostly built on top of the low level packages so if you want to dive into the details then you are in fact able to do so while not leaving Julia at all.\n",
    "\n",
    "<p align=\"left|right\">\n",
    "\n",
    " Package      | Description \n",
    " :----------- | :------------:\n",
    " `CUDAdrv`    | Access the CUDA driver API from Julia \n",
    " `CUDAnative` | Compile and execute native Julia kernels on CUDA hardware \n",
    " `CuArrays`   | Fully-functional GPU arrays similar to normal Julia Arrays\n",
    "\n",
    "\n",
    "The package `CUDAdrv` handles interactions with the GPU driver and provides some convenience functions e.g. to show the available devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{String,1}:\n",
       " \"Tesla P100-PCIE-16GB\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDAdrv\n",
    "map(name, devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `devices()` returns an iterator of device objects and `name` returns a string identifying the device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CUDAnative` package provides functionality for compiling (a subset of) Julia to GPUs. When loading the package, it will automatically initialize a device with an associated *contect*. We can use the context to see which device has been assigned to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): Tesla P100-PCIE-16GB"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDAnative\n",
    "device(CuCurrentContext())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package allows users to write GPU kernels but since GPU kernel writing is a demanding task we won't provide many details at this point but later hint at what is possible with `CUDAnative`.\n",
    "\n",
    "Instead, we'll focus on the `CuArrays` package which allows users to utilize the power of GPU through higher order functions on arrays such as `map`/`broadcast` and `reduce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Example: $\\pi$ on a GPU\n",
    "\n",
    "To show the power of GPU programming in Julia with `CuArrays` we can revisit the example of computing $\\pi$ as a Monte Carlo simulation.\n",
    "\n",
    "It is possible to draw random numbers very fast in parallel on a GPU if we use a specialized library. Hence, instead of drawing the variates one-by-one, we'll draw two arrays of random numbers in the GPU version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simulate_π_gpu (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CuArrays\n",
    "using CuArrays.CURAND\n",
    "simulate_π_gpu(n) = 4*sum(map((x, y) -> x^2 + y^2 < 1, curand(Float32, n), curand(Float32, n)))/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the function is compiled the first time it is called and GPU functions are quite slow to compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time simulate_π_gpu(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll load `BenchmarkTools` for the timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "@btime simulate_π_gpu(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For for `n = 10`, the CPU version is 300-400 times faster than the GPU version. Let's see the effecting using a ten times larger `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime simulate_π_gpu(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CPU version scaled roughly linearly whereas the GPU version is roughly unaffected by the 10x increase in sample size. This is explained by the large constant costs of memory operations on the GPU. Now lets try `n = 10^6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime simulate_π_gpu(10^6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll produce a vector of timings and plot together with the CPU timings for easy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_timings = [minimum(Base.@elapsed simulate_π_gpu(2^n) for i in 1:5) for n in ns];\n",
    "plot_timings(ns, cpu_timings, gpu_timings, names = [\"CPU\", \"GPU\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Arrays\n",
    "\n",
    "Julia has two different ways in which users can run Julia code interactively on multiple processes. The one we will use in this notebook is built into the core Julia distribution and should generally be chosen when you run Julia on your own hardware. Later, you using JuliaBox for exercises, you'll be introduced to another methods for using multiple processes. The two methods differ in how set up the processes but after the workers have been set up, most user facing functions are the same. The differences are monstly hidden to users but some of them are important to be aware of.\n",
    "\n",
    "As an introduction to multiprocesses in Julia, we'll now run simulate yet again. This time using eight Julia worker processes. First, we add the workers and load the `DistributedArrays` package to get access to arrays that can work accross multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nprocs() == 1\n",
    "    addprocs(8)\n",
    "end\n",
    "using DistributedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading package that should be available on all workers, it is important that these are loaded **after** the processes have been set up.\n",
    "\n",
    "Next, we define the simulation function. Notice that it is almost identical to the previous versions. The only difference here is that the random numbers are generated the `drand` function instead of `rand` and `curand`. As a prefix to the function, we have added `@everywhere`. This is necessary when defining functions \"at the global scope\" or \"top-level\" but is not necessary if a function is defined locally, e.g. as is the case for the `(x, y) -> x^2 + y^2 < 1` anonymous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere simulate_π_dist(n) = 4*sum(map((x, y) -> x^2 + y^2 < 1, drand(Float32, n), drand(Float32, n)))/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can run a few benchmarks to get an idea about the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime simulate_π_dist(10)\n",
    "@btime simulate_π_dist(100)\n",
    "@btime simulate_π_dist(10^6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the GPU version, the sample size appears to have little influence on the timings.\n",
    "\n",
    "Again we plot the tinings together with the previous timings to get a better understanding of the scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist_timings = [minimum(Base.@elapsed simulate_π_dist(2^n) for i in 1:5) for n in ns];\n",
    "plot_timings(ns, cpu_timings, gpu_timings, dist_timings,\n",
    "    names = [\"CPU\", \"GPU\", \"Distributed ($(nworkers()) CPUs)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we can see that the distributed version scales as expected but that the constant overhead is even more pronounced for distributed arrays compated to the GPU version. The cross-over point is around `n=10^6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading (experimental)\n",
    "\n",
    "Julia has native threading. However, the API is only being settled this spring and summer. What is presented below will probably become obsolete within the next year but it works and was the founadation for the single node parallelism in Celeste Project.\n",
    "\n",
    "Because the final API for Julia's threading hasn't been settled yet, threading hasn't been built into `map` and `broadcast` operations yet. We therefore cheat a little bit and define our custom threaded two-argument `tmap` to use in the comparison with the other implementations of the π simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Restricting elements to subtypes of AbstractFloat isn't strictly necessary\n",
    "# but makes it easier to compute the return type of the map\n",
    "# 2. Avoiding branches in function with the threaded for loop to help\n",
    "# the compiler (currently it has a huge effect)\n",
    "function _tmap!(f::F, y::Vector{T}, x1::Vector{T}, x2::Vector{T}) where {F<:Function, T<:AbstractFloat}\n",
    "    n = length(y)\n",
    "    \n",
    "    Threads.@threads for i in 1:n\n",
    "        y[i] = f(x1[i], x2[i])\n",
    "    end\n",
    "    \n",
    "    return y\n",
    "end\n",
    "\n",
    "function tmap(f::F, x1::Vector{T}, x2::Vector{T}) where {F<:Function,T<:AbstractFloat}\n",
    "    \n",
    "    # Check input args\n",
    "    n = length(x1)\n",
    "    if n != length(x2)\n",
    "        throw(DimensionMismatch(\"arrays must have the same length\"))\n",
    "    end\n",
    "    \n",
    "    # Allocate output array\n",
    "    y = similar(x1)\n",
    "    \n",
    "    # Call the threaded kernel\n",
    "    @inbounds _tmap!(f, y, x1, x2)\n",
    "    \n",
    "    return y\n",
    "end\n",
    "\n",
    "simulate_π_threads(n) = 4*sum(tmap((t, s) -> t^2 + s^2 < 1, rand(Float32, n), rand(Float32, n)))/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime simulate_π_threads(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime simulate_π_threads(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime simulate_π_threads(10^6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_timings = [minimum(Base.@elapsed simulate_π_threads(2^n) for i in 1:5) for n in ns];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timings(ns, cpu_timings, gpu_timings, dist_timings, threads_timings,\n",
    "    names = [\"CPU\", \"GPU\", \"Distributed ($(nworkers()) CPUs)\", \"Threaded ($(Threads.nthreads()) threads)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speedup from utilizing threading is modest. This is not surprising since the process of generating random variables hasn't been parallelized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A faster CPU version\n",
    "\n",
    "All the functions compared so far are based on allocating arrays of random numbers. By building up the computations from arrays, maps, and reductions, it is easy to utilize GPUs and distributed ressources. However, as we have seen, there is varying degrees of overhead associated with memory allocation.\n",
    "\n",
    "The Monte Carlo simulation actually doesn't require allocation of any arrays and we will finish off this demo with a version the doesn't allocate any arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that rand() is faster than rand(Float32)\n",
    "simulate_π_cpu_noalloc(n) = 4*sum(t -> rand()^2 + rand()^2 < 1, 1:n)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_noalloc_times = [minimum(Base.@elapsed simulate_π_cpu_noalloc(2^n) for i in 1:5) for n in ns];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timings(ns, cpu_timings, gpu_timings, dist_timings, threads_timings, cpu_noalloc_times,\n",
    "    names = [\"CPU\", \"GPU\", \"Distributed ($(nworkers()) CPUs)\", \"Threaded ($(Threads.nthreads()) threads)\", \"CPU noalloc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "DataFrame(\n",
    "    n = 2.^ns, \n",
    "    CPU_timings = cpu_timings,\n",
    "    GPU_timings = gpu_timings,\n",
    "    GPU_speedup = cpu_timings ./ gpu_timings,\n",
    "    Distributed_timings = dist_timings,\n",
    "    Distributed_speedup = cpu_timings ./ dist_timings,\n",
    "    Threaded_timings = threads_timings,\n",
    "    Threaded_timings = cpu_timings ./ threads_timings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.threadid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3-pre",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
