{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Deep: The Deep Neural Net\n",
    "\n",
    "So far we've learned that if we want to classify more than two fruits, we'll need to go beyond using a single neuron and use *multiple* neurons to get multiple outputs. We can think of stacking these multiple neurons together in a single neural layer.\n",
    "\n",
    "Even so, we found that using a single neural layer was not enough to fully distinguish between bananas, grapes, **and** apples. To do this properly, we'll need to add more complexity to our model. We need not just a neural net, but a *deep neural net*. \n",
    "\n",
    "There is one step remaining to build a deep neural network. We have been saying that a neural network takes in data and then spits out `0` or `1` predictions that together declare what kind of fruit the picture is. However, what if we instead put the output of a neural net into another neural net?\n",
    "\n",
    "This gets pictured like this below:\n",
    "\n",
    "<img src=\"data/deep-neural-net.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left we have 3 data points in blue. Those 3 data points each get fed into 4 neurons in purple. Each of those 4 neurons produce 3 outputs, which themselves are each fed into three neurons (the second layer of purple). Each of those 3 neurons spit out 6 values. Those 6 values then are the output of the neural network. This is a deep neural network.\n",
    "\n",
    "### Why Would A Deep Neural Net Be Better?\n",
    "\n",
    "This is a little purplexing when you first see it. We used neurons to train the model before: why would sticking neurons into neurons help us fit the data better? The answer can be understood by drawing pictures. Geometrically, the matrix multiplication inside of a layer of neurons is streching and rotating the axis that we can vary:\n",
    "\n",
    "[Show linear transformation of axis, with data]\n",
    "\n",
    "A nonlinear transformation, such as the sigmoid function, then adds a bump to the line:\n",
    "\n",
    "[Show the linear transformed axis with data, and then a bumped version that fits the data better]\n",
    "\n",
    "Now let's repeat this process. When we send the data through another layer of neurons, we get another rotations and another bump:\n",
    "\n",
    "[Show another rotation, then another bump]\n",
    "\n",
    "Visually, we see that if we keep doing this process we can make the axis line up with any data. What this means is that, **if we have enough layers, then our neural net can approximate any model**. The trade off is that with more layers we have more parameters so it may be harder to for the neural net to train, but we have the guarantee that the model has enough freedom such that there are parameters that will give the correct output. Because this model is so flexible, the problem is reduced to that of learning: do the same gradient descent method on this much larger model (but more efficiently!) and we can make it classify our data correctly. This is the power of deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
